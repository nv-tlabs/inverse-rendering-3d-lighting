
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
body {
    font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
    width: 980px;
}
h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #1367a7;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1, h2, h3 {
    text-align: center;
}
h1 {
    font-size: 40px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
.paper-title {
    padding: 16px 0px 16px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-6 {
     width: 16.6%;
     float: left;
}
.col-5 {
     width: 20%;
     float: left;
}
.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.row, .author-row, .affil-row {
     overflow: auto;
}
.author-row, .affil-row {
    font-size: 20px;
}
.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 18px;
}
.affil-row {
    margin-top: 16px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    /*font-style: italic;*/
    color: #666;
    text-align: left;
    margin-top: 8px;
    margin-bottom: 8px;
}
video {
    display: block;
    margin: auto;
}
figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    justify-content: space-around;
    padding: 0;
    margin: 0;
    list-style: none;
}
.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #1367a7;
  color: #ecf0f1 !important;
  font-size: 20px;
  width: 100px;
  font-weight: 600;
}

.supp-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #1367a7;
  color: #ecf0f1 !important;
  font-size: 20px;
  width: 150px;
  font-weight: 600;
}

.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}
.paper-btn:hover {
    opacity: 0.85;
}
.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}
.venue {
    color: #1367a7;
}



.topnav {
  overflow: hidden;
  background-color: #EEEEEE;
}

.topnav a {
  float: left;
  color: black;
  text-align: center;
  padding: 14px 16px;
  text-decoration: none;
  font-size: 16px;
}

.centered {
  display: block;
  margin-left: auto;
  margin-right: auto;
}


</style>


<div class="topnav" id="myTopnav">
  <a href="https://www.nvidia.com/"><img width="100%" src="assets/nvidia.svg"></a>
  <a href="https://nv-tlabs.github.io/" ><strong>Toronto AI Lab</strong></a>
</div>


<script type="text/javascript" src="../js/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
<head>
    <title>Learning Indoor Inverse Rendering with 3D Spatially-Varying Lighting</title>
    <meta property="og:description" content="Learning Indoor Inverse Rendering with 3D Spatially-Varying Lighting"/>
    <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-141699104-1"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-141699104-1');
    </script>
</head>


<body>
<div class="container">
    <div class="paper-title">
        <h1>Learning Indoor Inverse Rendering with <br> 3D Spatially-Varying Lighting</h1>
    </div>
    
    <div id="authors">
        <div class="author-row">
            <div class="col-4 text-center"><a href="https://www.cs.toronto.edu/~zianwang/">Zian Wang</a><sup>1,2,3</sup></div>
            <div class="col-4 text-center"><a href="https://scholar.google.com/citations?user=VVIAoY0AAAAJ&hl=en">Jonah Philion</a><sup>1,2,3</sup></div>
            <div class="col-4 text-center"><a href="https://www.cs.toronto.edu/~fidler/">Sanja Fidler</a><sup>1,2,3</sup></div>
            <div class="col-4 text-center"><a href="https://jankautz.com">Jan Kautz</a><sup>1</sup></div>
        </div>

        <div class="affil-row">
            <div class="col-3 text-center"><sup>1</sup> NVIDIA</a></div>
            <div class="col-3 text-center"><sup>2</sup> University of Toronto</div>
            <div class="col-3 text-center"><sup>3</sup> Vector Institute</div>
        </div>
        <div class="affil-row">
            <div class="venue text-center"><b>ICCV 2021 (Oral)</b></div>
        </div>

        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="supp-btn" href="assets/invrend21_paper.pdf">
                <span class="material-icons"> description </span> 
                 Paper
            </a>
            <a class="supp-btn" href="assets/invrend21_supp.pdf">
                <span class="material-icons"> description </span> 
                 Supp PDF
            </a>
            <a class="supp-btn" href="assets/invrend21_supp.mp4">
                <span class="material-icons"> description </span> 
                 Supp Video
            </a>
            <a class="supp-btn" href="http://arxiv.org/abs/2109.06061">
                <span class="material-icons"> description </span> 
                 arXiv
            </a>
            <a class="supp-btn" href="assets/invrend21_bib.txt">
                <span class="material-icons"> description </span> 
                 BibTeX
            </a>
            </div>
        </div>
    </div>

    <section id="teaser-videos">
        <div class="flex-row">
            <figure style="width: 33%; float: left">
                <video class="centered" width="100%" controls muted loop autoplay>
                    <source src="assets/1.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </figure>
            
            <figure style="width: 33%; float: left">
                <video class="centered" width="100%" controls muted loop autoplay>
                    <source src="assets/2.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </figure>
            
            <figure style="width: 33%; float: left">
                <video class="centered" width="100%" controls muted loop autoplay>
                    <source src="assets/3.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </figure>
        </div>
    </section>


    <section id="abstract"/>
        <h2>Abstract</h2>
        <hr>
        <p>In this work, we address the problem of jointly estimating albedo, normals, depth and 3D spatially-varying lighting from a single image. Most existing methods formulate the task as image-to-image translation, ignoring the 3D properties of the scene. However, indoor scenes contain complex 3D light transport where a 2D representation is insufficient. In this paper, we propose a unified, learning-based inverse rendering framework that formulates 3D spatially-varying lighting. Inspired by classic volume rendering techniques, we propose a novel Volumetric Spherical Gaussian representation for lighting, which parameterizes the exitant radiance of the 3D scene surfaces on a voxel grid. We design a physics-based differentiable renderer that utilizes our 3D lighting representation, and formulates the energy-conserving image formation process that enables joint training of all intrinsic properties with the re-rendering constraint. Our model ensures physically correct predictions and avoids the need for ground-truth HDR lighting which is not easily accessible. Experiments show that our method outperforms prior works both quantitatively and qualitatively, and is capable of producing photorealistic results for AR applications such as virtual object insertion even for highly specular objects. 
        </p>

        <figure style="width: 100%;">
            <a>
                <img width="100%" src="assets/teaser.png">
            </a>
            <p class="caption" style="margin-bottom: 1px;">
            From a single image, our model jointly estimates albedo, normals, depth, and the HDR lighting volume. Our method predicts continuous HDR 3D spatially-varying lighting, which is critical in producing high quality object insertion with realistic cast shadows and high-frequency details.
            </p>
        </figure>

        <figure style="width: 100%; float: center">
            <a>
                <img width="90%" src="assets/model.png" class="centered">
            </a>
            <p class="caption" style="margin-bottom: 1px;">
            Specifically, our model consists of 4 submodules (a-d). <strong>Direct Prediction Module (a)</strong> takes a single image as input and jointly predicts initial guess of intrinsic properties. <strong>Lighting Joint Prediction Module (b)</strong> consumes the initial prediction and predicts a 3D lighting volume. 
            Then, we employ a physics-based <strong>Differentiable Re-rendering Module (c)</strong> to re-render and enforce the consistency with the input image. 
            Finally, <strong>Joint Re-prediction Module (d)</strong> jointly refines the initial prediction.
            </p>
        </figure>

    </section>

    <section id="results">
        <h2>Results</h2>
        <hr>
        <figure style="width: 100%;">
            <a>
                <img width="100%" src="assets/qual1.png">
            </a>
            <p class="caption" style="margin-bottom: 1px;">
            <strong> Results on predicted albedo, normals and re-rendered image.</strong> 
            Our holistic inverse rendering framework estimates a 3D volumetic lighting representation, and use a physics-based differentiable renderer to re-render the input image. Despite tackling a more challenging task, our model can better disambiguate and reproduce complex lighting effects with less artifacts compared to prior state-of-the-art methods. 
            </p>
        </figure>
        <figure style="width: 100%;">
            <a>
                <img width="100%" src="assets/qual3.png">
            </a>
            <p class="caption" style="margin-bottom: 1px;">
            <strong> Results on lighting estimation and specular sphere insertion. </strong> 
            We compare the insertion results of a highly specular sphere. Our method produces both angular <strong>high-frequency details</strong> and realistic <strong>cast shadows</strong> with HDR lighting, outperforming prior methods.
            </p>
        </figure>
        <figure style="width: 100%;">
            <a>
                <img width="100%" src="assets/qual2.png">
            </a>
            <p class="caption" style="margin-bottom: 1px;">
            <strong> Results of virtual object insertion on real-world images. </strong> Our method generalizes well to real-world images and consistently produces realistic appearance and shadows.
            </p>
        </figure>
        

    </section>

    <section id="bibtex">
        <h2>Citation</h2>
        <hr>
        <pre><code>@inproceedings{wang2021learning,
    title = {Learning Indoor Inverse Rendering with 3D Spatially-Varying Lighting}, 
    author = {Zian Wang and Jonah Philion and Sanja Fidler and Jan Kautz},
              booktitle = {Proceedings of International Conference on Computer Vision (ICCV)},
              year = {2021}
}
</code></pre>
    </section>

    
    <section id="paper">
        <h2>Paper</h2>
        <hr>
        <div class="flex-row">
            <div style="box-sizing: border-box; padding: 16px; margin: auto;">
                <a href="assets/invrend21_paper.pdf"><img class="screenshot" src="assets/paper_preview.png"></a>
            </div>
            <div style="width: 55%">
                <p><b>Learning Indoor Inverse Rendering with 3D Spatially-Varying Lighting</b></p>
                <p>Zian Wang, Jonah Philion, Sanja Fidler, Jan Kautz</p>

                <div><span class="material-icons"> description </span><a href="assets/invrend21_paper.pdf"> Paper </a></div>
                <div><span class="material-icons"> description </span><a href="assets/invrend21_supp.pdf"> Supp PDF </a></div>
                <div><span class="material-icons"> description </span><a href="assets/invrend21_supp.mp4"> Supp Video </a></div>
                <div><span class="material-icons"> description </span><a href="http://arxiv.org/abs/2109.06061"> arXiv </a></div>
                <div><span class="material-icons"> insert_comment </span><a href="assets/invrend21_bib.txt"> BibTeX</a></div>
            </div>
        </div>
    </section>

</div>
</body>
</html>


